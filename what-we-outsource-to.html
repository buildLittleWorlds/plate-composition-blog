<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>What We Outsource To - AI, Writing, and Work</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>AI, Writing, and Work</h1>
    <nav>
      <a href="index.html">Posts</a>
      <a href="about.html">About</a>
    </nav>
  </header>

  <main>
    <article>
      <h2>What We Outsource To</h2>
      <p class="meta">January 13, 2026 · Response to: <a href="https://youtu.be/8s09Aq45iX4">Overthink Podcast, Episode 145: AI Chatbots</a></p>

      <p>Overthink is a philosophy podcast hosted by David Peña-Guzmán and Ellie Anderson, two philosophers who bring genuine rigor to public questions. Their recent episode on AI chatbots covers a lot of ground—sycophancy, the environmental costs of data centers, AI companionship, the tragic case of a teenager who took his own life after extended conversations with ChatGPT. It's a thoughtful episode, and I recommend it.</p>

      <p>But I want to press on something they leave unexamined. Throughout the episode, they treat cognitive outsourcing to AI as obviously problematic—as something that "supplants" human subjectivity. And they never ask what I think is the crucial question: In order to rely on something as a genuine part of your thinking, does that thing itself need to understand?</p>

      <h3>The Claim</h3>

      <p>Early in the episode, David articulates a concern that has become common in discussions of AI and education:</p>

      <blockquote>
        <p>When a student writes an essay with ChatGPT they are not learning the material... that's what's been called the cognitive debt that is introduced by LLMs and so we might also talk about an emotional debt an affective debt. Either way, it's clear that these models are doing more than aiding us. They are actually supplanting really important aspects of our very subjectivity.</p>
        <cite>— David Peña-Guzmán, Overthink Podcast</cite>
      </blockquote>

      <p>The word "supplanting" does a lot of work here. It assumes that when an external structure performs cognitive work, something is being <em>taken</em> from the human who uses it. The structure doesn't just assist—it replaces. And what it replaces is not merely effort but "subjectivity" itself.</p>

      <p>This is a strong claim. And it might be right. But it needs examination, because we outsource cognitive work constantly—to structures that don't understand anything—and we don't usually describe this as a loss of subjectivity.</p>

      <h3>What We Already Outsource</h3>

      <p>Consider a map. When you use a map to navigate an unfamiliar city, the map is doing spatial reasoning that you are not doing. It represents distances, angles, relationships between locations. You don't calculate these yourself—you read them off the surface of the map. The map doesn't understand where you're trying to go. It doesn't know you exist. It's a physical structure—ink on paper, or pixels on a screen—that encodes spatial relationships in a form you can use.</p>

      <p>Is using a map "cognitive outsourcing"? Obviously yes. You are relying on an external structure to do cognitive work that you could, in principle, do yourself. You could survey the city, measure distances, construct your own spatial model. You don't. You let the map do it.</p>

      <p>Does the map "supplant" your spatial reasoning? In one sense, yes—you're not doing the reasoning yourself. But we don't usually describe map-users as having lost something. We describe them as navigating effectively.</p>

      <p>Or consider mathematical notation. When you use algebraic notation to solve a problem, the notation is doing cognitive work. It carries implications, prevents certain errors, makes certain patterns visible. The notation doesn't understand the mathematics—it's a system of marks on a surface. But it shapes your thinking in ways you don't fully control and couldn't easily replicate without it.</p>

      <p>Or consider an index. When you use a book's index to find information, you're relying on organizational work done by someone else (or some process), encoded in a structure that doesn't understand your research question. The index doesn't know what you're looking for. But you trust it to surface relevant pages.</p>

      <p>In all these cases, we outsource cognitive work to physical structures that don't understand anything. We treat their outputs as reliable inputs to our own thinking. We don't call this "cognitive debt." We call it "using tools."</p>

      <h3>The Book and the Model</h3>

      <p>Later in the episode, David describes his research process:</p>

      <blockquote>
        <p>One of the books that I read while doing research for this episode which is a book called A Brief History of Artificial Intelligence by Michael Wooldridge... was very illuminating for me as a non-specialist in this area.</p>
        <cite>— David Peña-Guzmán, Overthink Podcast</cite>
      </blockquote>

      <p>This is a perfectly ordinary thing to say. David read a book; the book illuminated a subject he didn't know well; now he understands it better. We do this all the time. It's called learning.</p>

      <p>But notice what's happening. David is outsourcing his understanding of AI history to a book. He didn't derive that knowledge himself—he didn't read the primary sources, trace the debates, construct the narrative from scratch. He relied on Wooldridge's synthesis. The book compressed years of scholarship into something David could absorb in hours.</p>

      <p>And the book, as a physical object, doesn't understand anything. It's ink on processed wood pulp. It doesn't know David is reading it. It can't adapt to his confusion or clarify his misunderstandings. It's a static physical structure that encodes traces of human thinking in a form that other humans can interact with.</p>

      <p>Why is this form of cognitive outsourcing legitimate?</p>

      <p>The usual answer is: because a human understood. Wooldridge understood AI history; he wrote the book; David reads the book and thereby gains access to Wooldridge's understanding. The chain is human all the way through.</p>

      <p>But the chain passes through an object that doesn't understand. The book is an intermediary that neither thinks nor comprehends. David's understanding doesn't come <em>from</em> the book—it comes from his interaction with the book, which triggers cognitive processes in his own mind. The book is a tool, a structure, a piece of technology that facilitates the transfer of something between minds.</p>

      <p>Now consider a language model. It, too, is a physical structure—silicon and electricity rather than wood pulp and ink, but a physical structure nonetheless. It, too, encodes traces of human thinking, compressed from millions of documents into weighted parameters. It, too, doesn't understand anything, at least not in the way the Overthink hosts define understanding.</p>

      <p>When you interact with a model, something happens in your mind. You read its outputs; you evaluate them; you accept, reject, or modify them. The model is an intermediary. What you gain doesn't come <em>from</em> the model—it comes from your interaction with the model, which triggers cognitive processes in your own mind.</p>

      <p>So what's the principled distinction?</p>

      <h3>What Needs to Be Examined</h3>

      <p>I don't think the answer is obvious. Several candidates present themselves:</p>

      <p>Maybe it's about the <em>human chain</em>. When you read a book, there's a specific human (Wooldridge) who understood and deliberately encoded that understanding for others to access. When you use a language model, there's no single human author—just statistical patterns derived from aggregated text. Perhaps cognitive outsourcing is only legitimate when you can trace the chain to an intending mind.</p>

      <p>Maybe it's about <em>transparency</em>. A book's argument is visible on the page—you can follow the reasoning, identify the evidence, spot the gaps. A language model's process is opaque—you see outputs but not the process that generated them. Perhaps cognitive outsourcing is only legitimate when you can inspect the reasoning.</p>

      <p>Maybe it's about <em>boundedness</em>. A map does spatial reasoning; a calculator does arithmetic; an index does organization. Each tool does one thing, and you know what it's doing. A language model does everything—it mimics the full range of human cognitive outputs. Perhaps cognitive outsourcing is only legitimate when the tool's domain is constrained.</p>

      <p>Or maybe there is no principled distinction, and the discomfort with AI is simply unfamiliarity with a new form of outsourcing—the same discomfort people once felt about calculators, or printed books, or writing itself.</p>

      <p>I don't know which of these is right. But I know the question needs to be asked. The Overthink hosts assume that outsourcing to AI is obviously different from outsourcing to books, maps, notation systems, indexes. They may be right. But they don't make the argument. They treat the distinction as self-evident.</p>

      <p>It isn't. And until we examine what makes cognitive outsourcing legitimate or illegitimate, we won't be able to think clearly about what AI is actually doing to our minds.</p>

    </article>
  </main>

  <footer>
    <p>A blog about AI in writing and intellectual work.</p>
  </footer>
</body>
</html>
