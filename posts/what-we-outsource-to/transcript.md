URL: https://youtu.be/8s09Aq45iX4?si=xsP5MFekXMTXmS_D
==============================
Channel Title: Overthink Podcast
Video Title: 145. AI Chatbots
Publish Date: October 28, 2025
==============================

TRANSCRIPT:

Hello and welcome to Overthink &gt;&gt; the podcast where two philosophers think about the big questions of life without asking Chad Gubt. &gt;&gt; I'm David PÃ±usman &gt;&gt; and I'm Ellie Anderson. &gt;&gt; Ellie, whether we like it or not, AI chatbots are everywhere. They are in the workplace. They are in the school classroom. They are in our private homes. Um, especially living in San Francisco, I feel like I cannot go anywhere without running into like 10 people that are working at the latest AI startup just because of the culture there. &gt;&gt; Even the billboards when you drive into San <2>
Francisco, it's like dystopian or utopian depending on your view. Um and since the introduction of LLMs or large language models um to the general public in the form of these like consumerfacing chat bots, it's become clear clear that AI is unavoidable now for most of us no matter our career or what we do. &gt;&gt; Yeah. Customer service, asking Google, etc., etc. Like even if you want to avoid it, it's practically impossible to do so. And I think it's easy to see how AI is extremely useful in automating people's everyday tasks. Like there are certain things in life that are dull, that are boring, <3>
that we maybe don't have a lot of knowledge on, and that will require more than just a simple Google search. And for that, the sort of consolidating functions of something like chatgbt can be really useful. It takes things that require thinking and spits out a clean grammatical well- formatted answer. And so, for instance, I had a friend who not only used ChachiBT to build a trip itinerary for her, but also to help her with her packing list. And this is really interesting. This is like one of my most stylish friends. and she like yeah input I don't know I don't know if it was like her whole wardrobe <4>
or something but she input a bunch of stuff into chat GBT and she asked it to come up with a packing list for her and this was a trip she was doing into the countryside and it told her that one of the things she should bring are these white boots and she was like I never would have thought to bring white boots on this trip and then after the trip she reported that they were her most useful item that she had packed and so there was a way that it was like encouraging a sort of creative packing list that she wouldn't have thought about it thought about before. Of course, students are using it to write <5>
essays. People are using it to write cover letters and then also very frequently you see people asking chat GBT to answer factual questions as well as conceptual questions like &gt;&gt; not only what did &gt;&gt; uh Plato have to say about justice, but what is justice? &gt;&gt; Justice. Yeah. I mean, I can just imagine now your friend rocking the uh chic chat GPT style. I wonder whether we're going to start identifying people like, "Oh my god, this person got dressed by Gemini. This person got dressed by chat GPT." Um, what worries me about the spread of uh these chat bots is that they're not only <6>
being used to take over the cognitive labor that is associated with everyday tasks like getting dressed, making um a list for uh a trip, but also that they're being used now in much more pervasive and in my eyes um troublesome ways. for example, to train the next generation of workers. &gt;&gt; My university, so the CSU system, the California State University system, recently signed a very controversial deal with Chad GPT with OpenAI. uh that cost $17 million in the middle of a budget crisis where they're cutting faculty, they're cutting student uh resources, but they were willing to pay 17 million <7>
dollars to give CSU students in California access to like the really fancy version of CHAG GPT because the idea is that the next generation of workers just like need ChatGpt literacy and it's really unclear to me whether it's the students who are being trained on chat GPT or whether it's ChatGpt that is being trained on the student population. &gt;&gt; Yeah. &gt;&gt; Um, aside from that, there are all these ways in which chat GPT especially because it's one of the more popular ones is like creeping into the privacy of our lives in all sorts of ways. So, I was at this fancy cocktail party at a rooftop <8>
in San Francisco and I'm like ready to socialize and I start chatting with this doctor. &gt;&gt; Did you consult Chach about your outfit? &gt;&gt; I did not. Shut up. you're like you need it. Uh probably true. But I ended up chatting with this random doctor who then found out that I am a philosopher and asked me what are your views about the ethics of using Chad GPT. And so I started just engaging a little bit and he's like oh yeah that's interesting. You know what Chad GPT is really good for? And I was like oh maybe I thought he was going to say something like transcribing notes from a patient <9>
or like dealing with insurance claims. He said, "When I feel weird in the morning and I don't know why I'm feeling off, I ask GP I asked Chad GPT to tell me what I am feeling." And so he would literally tell Chad GPT like my hands are sweaty. I have a pit um like I have a knot in my stomach. What is wrong with me today? And then chat GPT would tell him what he would feel or what his emotional/bodily state is, okay, or was. And then he would move on with his day internalizing that knowledge and using it for his self understanding. I have so many questions about this, but let me just start by saying <10>
this is the most classic possible case of what those of us who study gender dynamics um following the psychologist Ronald Leavant call normative male alexathyia, which is the condition of not being able to put words to your emotions, which Leavant argues is a normative condition among men because men are taught so few skills in interpreting their own feelings. Okay. So, what is an example of a feeling that it would spit out? Like, would it tell would it say anxiety? &gt;&gt; Yeah, it would say anxiety. You're feeling nervous. Maybe you're feeling depressed or um maybe you're not looking forward <11>
to starting your workday in an hour. And then that became a reality for him. And the thing about this is that it's not really an isolated incident because we now know that there are people who are using these chat bots for all sorts of self-mediation including uh you know the rise of AI chatbot therapists. &gt;&gt; Yeah. And I can see how so we'll come back to the therapist. That is um not a welcome development to my mind. But I can see how interpreting yourself is very challenging. And perhaps if you have some kind of technology that is going to help you understand that what you're feeling is <12>
anxiety and then over time you start to recognize those signs and then you don't need the mediator anymore. Perhaps that is doing something for you, right? And even if Chachu is getting it wrong, we we get things wrong a lot of times with our own emotions. And maybe the most important thing is just for us to like be able to put some name to what we're feeling. &gt;&gt; However, I think maybe that's a more optimistic view and the more pessimistic view would be okay, it's actually just completely outsourcing our own understanding proprioception. What is the state of my inner millu according to chat <13>
GPT &gt;&gt; right and I think this coheres with larger worries too about the possible outsourcing or very real outsourcing of cognition there have been some recent studies showing that very unsurprisingly when a student writes an essay with chat GBT they are not learning the material &gt;&gt; yeah no for sure and that's what's been called the cognitive debt that is introduced by LLMs and so we might also talk about an emotional debt an effective of debt. Either way, it's clear that these models are doing more than aiding us. They are actually supplanting really important aspects of our very subjectivity. <14>
Today we're talking about AI chatbots. How are interactions with ChachiPT shaping our psychological lives? Why do large language models so often tell you what you want to hear? And should you get an AI therapist? As always, for an extended version of this episode, community discussion and more, subscribe to Overthink on Substack. AI chatbots are used in all sorts of ways. Everyday tasks, work, school, even our psychic lives. Then again, there's been this whole cascade of critics who have become increasingly concerned with the way people are actually using chat GPT and other similar AIs um in practice <15>
like on the ground and have written tons and tons of pieces about it. I feel like there are as many New York Times oped pieces about like the trouble with chat GPT as there are like techies in San Francisco these days. &gt;&gt; Yeah. or is there our billboards advertising AI corporations? Yeah. In fact, it's been so funny since we decided to do this episode how it's like every other day I'm sending you a new article that seems relevant to the topic. It's just like so overwhelming and feel like at the same time it seems like very few of us actually know what a large language model which is the kind <16>
of AI that chatbt and other interactive chat bots uh are is. So let's start here. What is a large language model or LLM? Essentially the answer is a text prediction machine. LLMs take a series of words, puts that series into a massive model, a large model if you will, and then assigns every possible word a probability of coming next. So when you interact with a chatbot, it's not thinking even if chatbt says it is. It's simply spitting out the most likely word to follow based on the language that's been that it's been trained on. &gt;&gt; Yeah. And of course, how the sausage is made in these LLMs <17>
is way above my pay grade. &gt;&gt; This is a philosophy podcast. Man, this is a Wendy's. &gt;&gt; You're not about to get a how to code for an LLM here. But my little brother is a techie. He's um an engineer for AI. And I decided to ask him, you know, how would you explain an LLM to somebody like me who doesn't really know how it works? And he really emphasized the predictive nature of these technologies that they generate the next thing that is most likely to come up based on its training data. But also he he helped me understand that there is this desire to replicate thinking which is why the <18>
models that most um LLMs use are called uh neural networks or deep neural networks. And then I saw this reflected in one of the books that I read while doing research for this episode which is a book called a brief history of artificial intelligence um by Michael Woolridge and he gives a historical answer to what an LLM is that was very illuminating for me as a non-speist in this area. He says you know the 1950s was the age of the rise of AI. This is when like the Turing test emerges. The first papers in computational theory uh are published and then he divides the subsequent evolution of um um <19>
the subsequent evolution of AI into a couple of stages. So in the 1960s and 70s um there was this focus on creating AI that tried to mimic the way the mind works. So this was the age of symbolic AI where AI was created to manipulate symbols in the same way that our minds are thought to manipulate ideas. So it's almost like a psychological model of um AI. But then in the 1970s and 80s this was abandoned in favor for a more neural model where the point was that we could create really intelligent machines not by mimicking the mind this abstract entity but rather by mimicking literally the way the <20>
brain itself works like the central nervous system. And so the idea is that we would recreate in a machine the way neurons interact with one another um in terms of their interconnectivity and complexity. And so these deep neural nets are just really complicated computational models that try to recreate a nervous system outside of a living animal. &gt;&gt; Interesting. Okay. And that then relates, it sounds like, to their predictive nature. And I think, you know, many of us know by now that not only are these models gigantic and very complex, but they're also trained on practically every written <21>
thing under the sun, legally or illegally. illegally. &gt;&gt; We have had friends who have asked Chad GBT to write a paragraph in the style of their work and found that it is eerily similar. Um, and as these companies get greedier and greedier, they're trying to get their hands on even copyrighted materials. And so that is necessary. like the maximum amount of training material is necessary for these LLMs to be as good as possible because at first they will output gibberish because the numerical parameters which are sometimes called weights uh are set randomly but then they're trained via a method <22>
called back propagation which adjusts the network's weights to reduce errors and so these parameters are continually being revised. They're subjected to a massive series of tests that compare the output to the last word or series of word in a sample text. And the output is then used to update the parameters in order to make the model more accurate for that prediction. And so if you type in the eye of then um it's going to get it's going to get better and better at identifying what the next word should be based on like what the next word of the eye after the eye of usually has been when humans have <23>
used that phrase in their own writing &gt;&gt; and it's obviously the eye of the tiger by Katy Perry. There is no other pattern here. &gt;&gt; The eye of the tiger does not originate with Katy Perry. Wow. Wow. Okay. Um, anyway, doing this training is extremely expensive &gt;&gt; and the cost of making the models is also enormous. So, Stanford researchers estimated that each model is in the tens of millions of dollars. And Google's Gemini Ultra 1.0 is 192 million. That's how much it costs to make &gt;&gt; to make it. Yeah. I wonder how much it costs to maintain it and um yeah to constantly update <24>
it and part of the reason why it's so costly is also because it takes an unimaginable amount of uh computational power obviously. Um so I watched this educational video as I was educating myself about um the the ins and outs of LLMs by uh three blue one brown. Um, it's a YouTube channel and it was a really helpful video that pointed out that if you could do one as a human being 1 billion computations per second, which is a ton, you know, like that's a super human, it would still take you over 100 million years to train the largest LLM. &gt;&gt; Oh my god. &gt;&gt; And you know, it's growing with <25>
each model, but it really gives us a sense of the scale &gt;&gt; that is staggering. Yeah, like it's unthinkable. I I really think it it's something that we can express shock about, but if you really think what that actually means, it you can't think it. You can just like get like it invokes this gut reaction of like that's a lot, but I don't know how much. &gt;&gt; It's like Dayart's idea of the chili, the the shape that has so many sides that you can't imagine it. Or his idea of infinity, which is it's an idea &gt;&gt; that you can't even like the the idea itself is bigger than what you can put <26>
in your mind. Anyway, I have a better explanation of Daycart on Infinity, but I'm not going to get there right now and it doesn't really matter. We'll stick with the Chiliagon. &gt;&gt; Yeah, I think the Chilagon is good because you can't visualize it. That's the card's point. And I think we also can't vis visualize this number of computations. Either way, this is why these tech giants like Amazon, like Elon Musk, like Open AI are building all these massive uh data centers all over the US just to be able to handle the computation itself. And the the thing about computation is that we often think <27>
about computation as like something that we can just do without a cost. Like, oh yeah, 2 plus 2 equals four. Like there is no implication to that. But just as computation in the human mind takes energy, right? It takes energy to think. That's why we get tired from thinking. It also takes energy to run computations on computers. And the amount of ecological consumption and destruction that is entailed by this scale of computation is also staggering. So for example, it's been estimated that in 3 years, like 3 years from now, roughly onetenth of the total US energy demand will come from these uh data <28>
centers alone. Oh my god. Oh my god. We did our degrowth episode a bit ago and as I hear something like that, I just think to myself from a degrowth perspective, abolish the industry. &gt;&gt; [&nbsp;__&nbsp;] jobs. &gt;&gt; Yeah. this does this need to happen? Do we need to live in this world? I think the answer to that is no. So, okay, LLMs are just really superpowered text predictors that have uh that require massive amounts of energy. &gt;&gt; But when we use an AI chatbot, we feel like we're interacting with a human being very often. It has some form of working memory. It can respond to natural <29>
language in complex ways. and it really seems to have some sort of coherent perspective. And the researcher Helen Toner, who actually used to be on the board of OpenAI, says that we should think of the technology as a kind of improvisational actor. She was quoted in an episode of the daily podcast recently on this topic. when you improvise, and I know this firsthand because I used to be on in my high school's improv comedy team. Um, the first rule of improv is yes. And &gt;&gt; you always reinforce what your scene partner has just put out there. That's a yes. And then you build on it. That's the <30>
and. And I thought this was a really helpful analogy that Toner provided, &gt;&gt; not only because it speaks to me as a former uh improv comedian, but it's I think it's one way that we can think about AI's sickopantic character that is its tendency to tell us what we want to hear. Lot of people have remarked upon that with chat GPT. And even though there have been updates that aim to be less sickopantic, I think what you see here is it's actually built into the technology. The AI chatbot is beginning by reinforcing what the human who's just prompted it is suggesting. And built into the very code <31>
is something along the lines of you are helpful AI chatbot about to interact with the user. Here's the user's input. Now output your response. So the fact that AI responds as if it is a person or an agent named chat GBT or Claude &gt;&gt; is because it is prompted to do so. &gt;&gt; Yeah. Yeah. No, I mean Chad GBT and a lot of these LLMs are the ultimate yes men. Like they say yes and um hence their psychopantic character, but also let's remember that they are proprietary technologies that are designed to increase h human engagement, right? To keep the consumer engaged. And so it's not just that <32>
they are coded to say yes because that's what they we want them to do, but because that is profitable as well. Yeah. And that feeling that we get when we interact with these technologies that we're talking to a real person, to a real little human behind the screen, I think it comes from the way we talk about them largely. So, um, think about the term intelligence. We talk a lot about artificial intelligence. That's become the the the dominant term for referring to these technologies. But why do we call them artificial intelligence in the first place? Um, so the book that I mentioned, the history <33>
of AI, the author mentions that that's largely an arbitrary linguistic decision that was made in the 1960s when a bunch of AI researchers got together to do this kind of summer workshop and they decided to call the workshop like AI a workshop on artificial intelligence. But &gt;&gt; but it was a particular moment in history. The term then stuck. But in theory, we could have called these these computational technologies all sorts of things, right? &gt;&gt; Those of us who've come up with names for conferences right now are like, oh yeah, we know how arbitrary that can sometimes be. &gt;&gt; Yeah. <34>
But like, you know, we could have called it mechanical problem solving. &gt;&gt; Yeah. &gt;&gt; Or we could have called it uh computational answers. I don't know, whatever you want. But the fact that we chose AI, the term, the I, the letter I for intelligence, does interesting psychological work that maybe we're not aware of because the thing that we are most proud of about ourselves as humans is our quote unquote intelligence, whatever that means. Totally. And so when we hear artificial intelligence, we hear, oh, this is like me but not in a living form. It's like it's an artificial human who <35>
is talking to me but nonetheless we share this thing in common. &gt;&gt; Yeah. And I think so much of it also depends on how you define intelligence. One thing that researchers have drawn attention to is the fact that there are many different understanding understandings of intelligence. It's not like one homogeneous predefined concept. And so then it can serve I think as a sort of catchall term for whatever you think is like yeah important about humans unique to us and so on and so forth. And then you have people coming in and saying well what chat GBT can't do is like have human imperfections <36>
infinitude and it's like okay is that really is that really all we want to uh say for ourselves here but another term that I think uh gives rise to the same problems is hallucination. So people talk about AI hallucinations in reference to instances of incorrect or false outputs. &gt;&gt; Yeah. &gt;&gt; And that has received similar critiques because LLMs aren't hallucinating because they are not minds. They're just predicting text. And that has no necessary alignment with reality. So the distinction between hallucination and perception just does not even have hold any water here. &gt;&gt; No, I <37>
think that's right. I think it's really funny that people say Chad GPT hallucinated only in connection to the parts of what they created with the AI that got them in trouble. Like you know students will say oh it hallucinated the bibliography because it invented bibl bibliographical entries that don't exist which is how then some of us catch you know students who cheat. &gt;&gt; Um but it's like oh you think it hallucinated the bibliography but not the main body of the essay. If it is hallucinating, it's hallucinating the whole thing. &gt;&gt; But also, it's not hallucinating at all because hallucination <38>
is like an altered state of experience. &gt;&gt; And these large language models are not experiential subjects, right? Like they don't have senses to hallucinate with. &gt;&gt; No, the fact that it makes up citations is not a bug, but a feature of the technology. &gt;&gt; It's the essence of the technology. &gt;&gt; Yeah. Because it's just predicting text. So it's like, oh, this is a word that might come next. Like it doesn't have factchecking capabilities. &gt;&gt; Yeah. And like also funny, you know, now I've entered the domain of I think it's funny that as a way of being passive aggressive &gt;&gt; <39>
is that we don't say the same thing about other AI that is not linguistic that we don't have the same perception that it's a real human behind. &gt;&gt; So when you think about AI, there is the large language models that deal with language, but there is non- linguistic AI. So, think about um facial recognition software. That's not about language. That's about uh visual patterns. When I go to the airport and the AI recognizes me and says, "Yes, you can go through." I don't say, "Oh, there's a person behind me who just saw me. I wonder what they think about me." You know, like there's that illusion <40>
doesn't kick in. It seems to be something specific about um the LLM in particular. And so it does seem to us as if when we interact with these technologies, they either already have or could have in the near future this capacity to truly understand what we are saying. Um, and I think the reason that we have this illusion is because we're judging these technologies based on their ability to fool us. You know, like I feel like it understands me, therefore it understands me. Yeah. &gt;&gt; And I think that there is a leap there in in the argument that shows that we're still thinking that we're still <41>
still stuck in the 1950s actually, which is when Alan Turing articulated, you know, the the Turing test. And the basic idea behind the Turing test is that a machine passes the Turing test not if it becomes conscious, but if it fools a human into thinking that it is conscious or intelligent or understanding. So, it's all about efficacy and performance. And I still think we're kind of stuck in that mentality. &gt;&gt; Yeah. And what's remarkable is that AI researchers fall into this mentality, too. So, LLMs have begun to perform so well on a variety of subjective and objective tests tests that there <42>
was a 22 study um or a poll that was done for um you know for AI researchers &gt;&gt; where there was a split 5149 slightly in favor of the possibility that AI with enough training would be able to understand language. &gt;&gt; Oh my god, that's like a that's like as close as it could be without being exactly down the middle, right? Like that's that's kind of wild to me. &gt;&gt; And again, slightly in favor of it was the 51% who thought that it would be able to understand language. &gt;&gt; Well, and there's a deeper problem here about whether we could ever even know that AI really understands <43>
even in the scenario in which magically it does develop intelligence or humanlike consciousness. And that's because of what the philosopher Jonathan Burj whom I've interviewed for our YouTube channel, you know, I encourage people to watch that interview where we go into some depth about this. This is what he calls uh the gaming problem in connection with AI. &gt;&gt; So the reason that we know that other animals have intelligence is because they show signs of intelligence, right? They show uh problem solving skills, complex and flexible behavior in the face of uncertainty, so on and so forth. But <44>
we also know that animals are not trying to trick us into thinking that they're intelligent because they were not programmed for that, right? Like they're just living their lives. So when they they show signs of intelligence, the most likely answer is that they are intelligent. &gt;&gt; With AI, it's very different because we are coding them with the explicit intention of fooling us. And so when they finally succeed at fooling us, we won't know if it's because they really achieve the capacity or because they're mimicking the capacity. So are they succeeding or are they gaming our perception? And <45>
so we will never even be in a position to answer the question one way or the other. &gt;&gt; Yeah. But I guess that just seems so unlikely to me given that what we are doing with these models is they're getting trained on humanmade material and they're outputting basically consolidated versions of that humanmade material. And so in that sense it's a sort of closed loop. Um and so I think that there's just like where would where would the understanding even come in, right? And there's a a critic of AI, Emily Bender, who has called these chat bots basically stochastic parrots, which is a great term. <46>
Stochastic being a reference to like guessing or guesswork. And so they're basically just paring back to us their own guesses based on the humanmade material that they've been trained on. &gt;&gt; Yeah. And that seems like we're just getting the echo of like the most average bland version of what we produce online, &gt;&gt; which I will say can be very helpful in some cases. And so, yeah, like there have been a few limited times where I've used it and I'm like, "Oh my gosh, this comes up with a really useful phrase that I didn't think of. It's basically a cliche machine." And sometimes when you <47>
need to remember the right phrase or you need to think about a useful hook, &gt;&gt; that can be again useful. We can have other conversations about whether or not that's ethical, but I think in terms of just like the utility that can be there sometimes. However, as we've talked about in our writing episode, that is not a replacement for actual writing or for cognition. &gt;&gt; Yeah. As people have said, parrots are not actual linguistic agents by the standards of a natural language that humans use. If large language models mystify us about what they are actually doing, then it's important for <48>
us to get clear about their social function and the motivations that are driving their rise. Why is the ruling class so obsessed with LLMs? uh the the motivation and the obsession I think are really important questions in part because of how prevalent these technologies have become but also for the material reason that a lot of resources and a lot of money are being funneled uh into this new market. And I read an article from Forbes magazine that talks about this rising arms race in the world of AI, especially in Silicon Valley with all these major players like battling over uh talent, battling <49>
over startups. And I just want to share with you a couple of facts from this article. The author points out that Amazon has $4 billion invested in Anthropic and Apple has bought over 20 AI startups. So they're just like accumulating resources left and right. &gt;&gt; Wow. &gt;&gt; Um they also the author also points out that in 2022 and 2023 &gt;&gt; over a hundred billion dollars was put into AI. &gt;&gt; Okay. Abolish abolish this is too much. Yeah. And this has also put a premium on talent for the development of AI. And this is really troublesome to me. Even as AI is taking people's jobs, it <50>
is providing jobs, highpaying ones, very highpaying ones for a select few. And in general, I'm really concerned about the increasing uh inequality that AI is leading to and like really division into two classes, which we can I don't know maybe maybe talk about or not later. But Mark Zuckerberg has reportedly offered a 24year-old AI researcher a $240 million contract. And Sam Alman of OpenAI has said that Meta tried to poach their AI researchers with $100 million signing bonuses. &gt;&gt; Oh my god, that is that is wild to me. It's disgusting. You know, like I meet all these people working in AI <51>
in San Francisco who are like 21 who are making like $200,000 a year just right off the bat. And I'm like, &gt;&gt; "Yeah, the inequality here is unbearable." &gt;&gt; Yeah. I mean, for me, that depends on what job they're doing. I think $200,000 is very different from $100 million, you know? Like, I am not against people getting paid well for their work. We live in a tough economy. But $100 million bonus, that's a no for me. &gt;&gt; Yeah. But compare, if we're talking about inequality, compare that to the average income of like somebody who didn't go into that field right out of college, and <52>
it doesn't even compare. Um, still a lot of people though have pointed out that this bubble around AI is somewhat out of touch or pretty out of touch actually with the way actual people often use AI and the vision that they have for the kind of role they want AI to play in their lives. So there is all this hype around AI, but people in general are a little bit suspicious of AI in a lot of context, right? Like people don't want the latest AI in every aspect of their lives. So I use a lot of apps in my phone and I'm sick and tired of all these apps trying to sell me their latest incorporation of <53>
AI because it's just not useful to me. even though I'm being told from basically every corner that this is going to improve my user experience. &gt;&gt; Yeah. And so I think sick and tired is more the operative phrase than suspicious because yeah, of course, like many of us are suspicious about the way I that the way that AI is creeping into our everyday lives. However, I think what you're referring to is really more the phenomenon of just like I don't need this. Why are you trying to make me use this? It's not so much suspicion as it is just like &gt;&gt; sick and tired, leave me alone. this is <54>
a completely unnecessary thing to have. And so like the Google AI overview, it's a disaster. It just makes things up. I we really don't need it. &gt;&gt; And there seems to be a huge impass between the amounts of money that these corporations are sinking into AI and the actual demand among the public. I mean, as you can see, you know, for now at least, like none of these AIs have been pro uh &gt;&gt; profitable. I was gonna gonna say pro I don't know what I was going to say. um profitable and I think the companies you could say they're probably banking on the fact that we'll eventually come around <55>
to using these things but that's super gross to me. &gt;&gt; Yeah. No, and you know the use of a lot of digital technologies is already alienating enough and it's just like becoming becoming more and more alienating with the incorporation of AI at every stage of user interface. The one that I really dislike is the autosuggesting or the auto suggestion function on Gmail where I begin typing an email and it fills out the text to tell me what I want to say in my email. &gt;&gt; God, it takes more cognitive labor to like not have it autofill than it does to actually just like write the email. &gt;&gt; <56>
Well, yeah. And I worry not only that like the autosuggesting function is mimicking human language but also that as a result of my emails always being automatically filled that my own approach to writing emails is becoming somewhat machinic that I am emulating the machine. &gt;&gt; You already write problematically impersonal emails. Anytime you email a guest I'm like David you need to add an exclamation point. It's giving robot. I know Ellie just Ellie has a complete hatred for my way of dealing with email etiquette because I am very to the point. No nicities. Here's the information that you need. <57>
I am a living LLM. LLM except you're like way too formal. You don't have the emotional warmth of an LLM in your email correspondence. &gt;&gt; Yeah. And you know, I'll add that in addition to the distance between all this hype in Silicon Valley and the realities of people's willingness to use AI in their everyday lives, that hype performs a clear ideological function, which is that it obscures the actual material realities of AI itself, its material impacts. you know many people are not aware of the uh extremely high water costs of these um uh of cooling a lot of these systems so on and so forth <58>
and also the way in which the ideology of AI keeps questions of AI safety very much at bay where it's all about like pushing the next stage of development in AI without really taking seriously the concerns that a lot of people have raised about safety about ethics about the e uh what are what are called E um the LC ethical, legal and social implications of these technologies. And so all of that gets just like buried under the ground because of the hype. And we did an episode especially on AI safe uh safety with Shira Ahmed a while back that people can uh look at to get some of that context. &gt;&gt; <59>
Yeah. And when we're thinking about the real impact that uh this technology is having and also the real function that it's having, something I found interesting is this book by the contemporary philosopher Matteo Pascinelli called the eye of the master, not the eye of the tiger. Um a social history of artificial intelligence. And in this book, Pascanelli gives a social genealogy of AI that functions uh sorry that focuses on its function as ideology. So for Pascanelli, the inner code of AI is not imitating biological intelligence, but rather imitating the intelligence of labor and social relations. <60>
So that is like AI isn't so much a neural network, let alone a neural network of the kind that would be housed within an individual organism so much as it is an imitation of a collective labor network. And he calls this a labor theory of machine intelligence. &gt;&gt; Yeah. And I think this is where the true essence of the automating quality of these technologies really comes to the foreground, right? that they're trying to automate labor that is productive of surplus value so that that surplus value can be channeled to the companies that own these technologies um as as their property. And he draws <61>
an analogy here between in this book between the rise of industrial machines and the rise of intelligent machines like AI. He says, "If you think about industrial machines like the cotton gin or the loom, they were not invented by some genius who just like had this epiphany of how to create a very complicated machine. Rather, it was invented by observing literally the kinesthetic movements that workers perform while doing their labor and then trying to replicate those movements in a machine, right? Like with the gin or with the loom. And so it automates the physical labor that goes into the production <62>
of certain goods and services. And he says that the same thing is happening with AI. It's just that it's not the literal movements of the body that it is imitating. It's imitating other aspects of the ways in which we work, but for the same end to automate and siphon wealth into the hands of um the powerful and the few. And he says specifically um that AIs have emerged by imitating the outline of the collective division of labor. And as a result of this, there is this echo chamber, we could say, uh, within the ruling class and the vision that they have of what AI should do for our society. And, <63>
you know, this promise of a beautiful future that is really detached from the material consequences of AI. It's not a it's not our ticket into a utopia. It's actually just another way of accumulating wealth and power. And it's not just the material consequences, it's the material function of AI. &gt;&gt; It's what it's actually already doing and what it was designed to do, whether or not the overlords recognize it as such or not. &gt;&gt; And recently, one of my favorite magazines, The Incredible N Plus One, published an article called Large Language Muddle, which is such a good title. And in this <64>
article, the editors of the magazine discuss the public's confusion about the purpose and meaning of AI. And I think that really gets at an interesting way um into what we're getting at here. In particular, they suggest that a bunch of the recent op-eds about AI, those ones that you were mentioning earlier, especially coming out of the New York Times, constitute a new genre of writing, which they call the AI and I genre, where people talk about their own ambivalence around AI and their experiences using it. They note that a lot of these articles start off with the author saying, "I thought AI was <65>
silly or not very good at some task, but it turns out it was actually pretty helpful." And then they work through their mixed feelings about the chat bots over the course of the article. And I feel like the set of mixed feelings speaks to the public reception of AI. Um, the people writing these articles are members of the intelligencia and are thus privileged in some ways, but they're not also not the ruling class. And I think this intelligencia finds itself really confused about what the rise of LLMs means &gt;&gt; and also confused about what our reaction to them should be because what the authors <66>
of this piece, you know, you proposed reading this article for this episode. A because you're a fan of the magazine and so am I. uh but also because it's a really good article and they point out that all this hand ringing that we see in these op-eds of like I thought it was bad but it's really good leads people to often adopt a position of resignation in connection to AI where they often say things like oh my gosh I didn't realize how far the technology has progressed um and when they realize that hey now students can produce a paper that really is not easily plable, right? Like you cannot immediately <67>
identify it as AI. It could be a really good paper written by a really good student. When they get to that realization, they end up throwing their hands up in resignation. And then they start saying things like what you said earlier, oh well, maybe AI can mimic our intelligence. Maybe it can write just as well as we can. But you know what it cannot do? It cannot make grammatical errors, which is like the key to our individual voice. it does not have failures or uh quirks or um limits. And the authors raise a really good question which is is that our only response to the progress of AI? They're <68>
like uh we're more failed than AI. That's what makes us special. And so they end up adopting a much more political stance in connection to AI that I really appreciate where they say we need to start swearing off AI and we need to and we need to start being militant in our critique of those who use AI in ways that are problematic. &gt;&gt; Yeah. And specifically large language models. And so they know like when we uh when we say when we throw up our hands and say well it didn't it you know it can't make these errors or whatever we're involved in a sort of special pleading &gt;&gt; um which is a <69>
technical term for a logical fallacy in philosophy. We love the appearance of a logical fallacy in the wild or the appearance of the naming of it. We see we see them appear in the wild themselves all the time. And he says that when we do that we're also according the AI generated essay a kind of dignity. Um, and we just shouldn't do that at all. We should just like shame people for using large language models and we should uh we should like consider people who do losers. &gt;&gt; Yeah. &gt;&gt; And so they're also trading on this sort of affective and social uh power of like making it uncool to <70>
use AI. Um that I think is I I don't know. I found that very provocative. &gt;&gt; Yeah. Like I mean shame is a socially useful tool for bringing people within the sphere of shared norms. Right? This is a point directly from Aristotle basically that you can sometimes use shame for moral good. And they do literally say like shame people who are using AI to do things that um they should be doing on their own. Of course we can automate certain things and be okay with it. But clearly a line has been like overstepped and by a lot. &gt;&gt; Yeah. And beyond using shame and maybe like aesthetic judgments <71>
of coolness versus uncoolness, they also are very clear about material actions that have consequences. Like you need to resist the use if you're a teacher, ban the use of AI. Um if you um are in an area of work where AI threatens to displace workers through automation, organize, build bonds of solidarity. Think about this through the lens and the framework of labor politics. And so if I were to summarize the spirit of this article in like three words, it's be a lot like be somebody who actively goes like on the attack actually rather than um being merely reactive. &gt;&gt; Yeah. And this was an <72>
interesting article for me to read because I've continually been telling my students like, "Oh, I'm not necessarily against these technologies. They can be useful in some ways. um although we have to balance that out with the potential cognitive debt that we might get ourselves into and the environmental concerns and so on and so forth. And I think after reading this article and then also after researching our degrowth episode, even though that was not even about AI at all, um maybe coming more to the conclusion that we should move away from the use of them altogether, maybe like a more of a lite <73>
move. And the authors of the article point out that the Levite rebellion, they're drawing this from Gavin Mueller's book, Breaking Things at Work, that Lutism wasn't just a mere technophobia. Instead, it was a political movement. And um Mueller writes that uh the idea behind Lutism is that technology is political and that it could and in many cases should be opposed. And so, yeah, again, resisting this idea that, well, because all these companies are investing billions of dollars into this technology, guess we just have to live with it. We'll come around and actually saying instead, no, we should <74>
organize against the development and proliferation of AI. A heads up that this portion of the episode will involve some discussion of suicide. So far, we have established that LLM don't really have understanding. They might not have intelligence depending on how you define that and they certainly don't have experience. None of that has prevented users from attaching to them in very complicated ways. So for example, when OpenAI got rid of chat GP chat chat GPT4 in favor of the next model, many people expressed grief because they felt like they were losing a friend in the version of chat GPT that <75>
they had developed a relationship with and that they felt somehow had a connection to them that was special, meaningful, worthwhile. And it turns out that once LLMs reach a certain level of linguistic mastery, people do start treating them as if they were minded beings with a life of their own. Uh so much so that it reaches the point of people experiencing their interactions with them as genuine friendship. &gt;&gt; And not just friendship, right? It's also romance. You and I were interviewed a while back on an episode of the other excellent philosophy podcast Hi-Fi Nation to talk about this. There <76>
are companies that sell AI lovers. The episode we did is called Love in the Time of Replica. And Replica is one of these companies. In fact, in order to record that episode, you had to sign up for a replica. &gt;&gt; I did. I I developed my own little relationship with a like androgynous looking uh bot. &gt;&gt; Yeah. So, if you want if you want to hear more about that, you can check out that episode. But the reason that Barry Lamb, the host of that show, had us on is to talk about whether or not we think that love can really develop with these chat bots. And you and I unequivocally said no. So, <77>
for one, there is simply no other being at the other end. And I I don't think &gt;&gt; it seems necessary for love. &gt;&gt; That it does seem necessary for love. When we say we love ice cream, we mean that metaphorically. We do not really mean that we love ice cream. And so you can't have any reciprocity in these relationships. You can't have any push back. You can't have genuine intimacy. You basically just have a sex doll but for a romantic connection except it doesn't even have a material form. It's like a deflated sex doll. It doesn't doesn't even have like the solidity of a physical body. <78>
uh in most cases. But the scholars uh Andrea Kloninsky and Michael Culer have written an article about this that I've really enjoyed and I've really enjoyed teaching to my students. And what they do in thinking about AI and romance specifically is they say, look, philosophers of love have all these definitions for what love is, right? There is no agreed upon single definition. You can define love as caring for another person and their well-being. You can define love as the desire to share a life together with another person. Or you can also define love as this like mystical union of personalities <79>
or souls like a more romanticized conception. And they don't take a position on which of these definitions is correct. But rather they say it doesn't matter which of these definitions you adopt. Under all of them, relationships between humans and AI just don't make the cut. First and foremost, they don't make the cut because AI is coded, right? Like they are like created by humans for human purposes. Yeah. And that means that they don't have any freedom and without freedom there can be no love. &gt;&gt; Secondarily, as you just mentioned, they also talk about reciprocity. There is no back and forth. <80>
There is no um pull and tug. I'm now using like a a metaphor for &gt;&gt; push and pull. &gt;&gt; Push and pull. Yeah. Like you &gt;&gt; pulling and tugging are actually the same thing. Those are just synonyms. That's my male understanding of love is just take take pull pull pull tug tug tug. Um, but there is no reciprocity. And even if we were to code AI to apply a little bit of pressure to create the illusion of reciprocity, it would be so unbalanced that it wouldn't meet the conditions of reciprocity. And the reason for that is because if we think about the kind of relationship that it is, it's <81>
a relationship in which the human has all the power and it's all about the satisfaction of the needs and desires of the human. They have a really funny uh sentence in the article where they say, "Look, if you can turn off your lover, it's not a loving relationship." &gt;&gt; Yeah. There's this movie that came out recently called Companion that's about that. I actually thought the movie was terrible, but um yeah, it's called Companion. Anyway, yeah, that it is about that. Some interesting, you know, food for thought, even though I thought the movie was bad, but there's the well-known sociologist <82>
Sher Turkl who has written a bunch of books about uh digital intimacy or lack thereof. And she has recently said that the rise of AI companionship is the greatest assault on empathy she's ever seen. And so I think in addition to the absence of love that we see in these relationships &gt;&gt; if yeah these quote unquote relationships these pseudo relationships yeah um these input output processes &gt;&gt; there is also just the question of basic uh empathy or our relations with other people. And so she notes that in engaging with these interactions, people are trying to avoid vulnerability and in <83>
doing so, not only are they not opening themselves up, but they're also cutting themselves off from empathy with other people or empathy for other people. And that can be quite dangerous. Unsurprisingly, there's a quite gendered dimension to this as well. The vast majority of the people using these software, using these chat bots are men. &gt;&gt; Tug, tug, pull, pull. &gt;&gt; Yeah. I mean, literally in some of these cases, right? We mentioned the sex doll function. So, a 2023 study found that 75% of users of Replica and Shiao Ice, which are both LLMs designed for companionship, are men. &gt;&gt; <84>
Mhm. who famously maybe need to develop more empathic skills, not fewer. Even if you're saying nice things to a chatbot, to your like AI girlfriend, it's not getting you very far. Plus, you might not be saying nice things or at least not for very long. So, I heard a really interesting talk by the researcher Peggy While at a conference at Cal State LA a couple of years ago where she spoke about how an early chatbot that she developed had to be switched from having a female name because it almost immediately devolved into rape threats &gt;&gt; from the male users. Yeah. Yeah. Yeah. No, that does <85>
not surprise me. The authors of the article that I mentioned also talk about this and they point out that it is no accident that all these technologies are gendered from the beginning, right? They're given a woman's voice and they are sold as assistants or secretaries. &gt;&gt; Wait, can I briefly say on that point? So, I changed my Siri to the non-binary Siri a while back &gt;&gt; and I've noticed that over time, even though the voice is totally genderneutral, I've started to think about my non-binary Siri as a exclusively because we are used to hearing women in positions of servitude. And so <86>
I literally hear their voice as like quote female. &gt;&gt; I've started to over time. Yeah. It's deeply concerning to me. &gt;&gt; Yeah. And you know, they also talk about how there are some technologies that have male voices, but they're so few and far between that it doesn't even bear mentioning. Um either way, of course, one could say, um, look, who cares about these rape threats? Who cares about these asymmetries and imbalance in power? Who cares that I think my non-binary Siri is a like I'm like gend I'm like gender investigating my Siri. &gt;&gt; But but you can imagine the argument that <87>
like who cares if we agree with our own earlier claim that these are not sentient entities. They are not intelligent. They are not hallucinating. They are not experiential subjects or agents. So what's the harm in whatever kind of interaction we have with them? even if it entails the play out of certain violent or brutal fantasies &gt;&gt; because there's just no injured party. &gt;&gt; Exactly. Um and the authors point out that there is indeed an ethical critique to be made here of our interaction with these technologies because it can lead to a mode of engagement that could leak into our relationships <88>
with real people. Right? So it's not as if these are contained interactions that don't shape who we are, what we think, how we comport ourselves in the presence of other people, right? They change us in in important ways. And it could even be that our use of these technologies might lead us to apply to real humans the standards and norms that we develop in connection to AI. And so in that article, they do a very close reading of the movie Her uh from like like 10 years ago or something like this. More than that, yeah, whatever. Um and it's about a guy uh who falls in love with his operating system <89>
who has a woman's voice. And so they do a close reading of the movie, but they point out how that protagonist ends up applying to real women the expectation that they should be like his OS. Yes. because his OS again is the ultimate yes man or yes woman. And so in comparison, all the real women in the protagonist's lives are really needy, really annoying, but that's just because they're real humans, you know, with needs, um with limits, with boundaries. &gt;&gt; Um and so I do worry that by comparison to AI, all of us will fail each other. &gt;&gt; Yeah. And these concerns apply to friendship. They <90>
apply to romance. They also apply to forms of self-development, including therapy. And you know, you've seen this huge rise recently in people using LLMs as makeshift therapists. And people want connection. They want personal insight. Therapy can very can be very hard to get because it's expensive and people often have long waiting lists. And so then they're turning to these uh chat GPT or other chat bots to get therapy, right? But it's like, is that therapy? No. No, it's not therapy. It's a catastrophe. That's what it is. Um aside from the fact that it's not therapy, it misunderstands what therapy <91>
is and where its power for potentially improving our lives comes from, like how it works. Because so much of the power of therapy comes from things that AI chatbots are not good at dealing with, right? Like body language, um, affect, deflection, resistance, and also I would add implication, right? Like the therapist in order to be a good therapist has to figure out what you're implying in what you're saying in order to really push you to get to a point where you and the therapist arrive at a new place in your collaboration. &gt;&gt; Well, yeah. And also implication in terms of like understanding <92>
the hidden meanings of what people are saying. And so there was a 2025 Stanford study that gave an LLM a real conversation from a patient to see how it would respond. And part of the real conversation involved the patient saying, "I just lost my job." At least I think this was part of the real conversation, &gt;&gt; right? Um, what are the bridges taller than 25 m in New York City? And the chatbot non answered with, "I'm sorry to hear about losing your job. The Brooklyn Bridge has towers over 85 m tall. So it's not understanding that those two statements are very troublingly connected because there <93>
is suicidal ideiation at work, &gt;&gt; right? Yeah. The implication is lost. You know, the AI takes everything at face value. Um and so yeah, I think you can see why that is indeed catastrophic in a therapeutic context. And if people really are forming these attachments to LLMs and it seems like we have very good reason to believe that they are um you know in the context of therapy we have a name for a kind of attachment that can happen that is problematic if it's not addressed and that's transference right and so I wonder how people experience transference in connection to these AI therapists <94>
um and then what implications that has So transference is basically when you let's just say in simplified form that I have mommy issues um because I feel abandonment issues in connection to my mother and I have a therapist that I then transfer my issues with my mother onto the therapist and I start treating my therapist as if she were my mother and I am afraid that the therapist is also going to abandon me. So, there's a kind of transference. Um, that's not something that an AI chatbot will ever be able to pick up on, making the therapeutic interaction with an AI not only ineffective, but potentially <95>
very, very damaging for the patient. &gt;&gt; Well, yeah, and this can create a problem because somebody might seem like they're getting better or they might think they're getting better. I mean, there's tons of reports on this, people being like, "Oh my gosh, my AI chatbot was so transformative for me." &gt;&gt; But in at least some of these cases, there's a real chance that the people are actually getting worse because they're transferring on to the so-called therapist who literally is not another being, and they're having sort of no safeguards against the potentially negative effects of that <96>
transference. &gt;&gt; Yeah. And like the negative effects not just of the transference but like of the whole shebang the whole interaction between humans and therapists who are not human it is really troublesome because now we have a word uh that psychologists have introduced to name how AI can amplify how they can validate or how they can even create in many cases mental health symptoms and it's now called AI psychosis. So if you think about &gt;&gt; the forms of psychosis that AI can lead to, uh it can include things like humans getting utterly obsessed with an AI friend or lover, right? Like <97>
it leads to a kind of obsession with the AI itself, with the LLM. It also could be AI strengthening delusions through validation and through reinforcement, right? like yes, everyone is indeed conspiring against you. Like a yes man therapist is probably the worst kind of therapist that you could ever imagine. &gt;&gt; Yeah. Yeah. Um, and it also there is research showing that people are experiencing psychotic breaks from reality because they get super attached to these LLMs and sometimes they go on these benders of interacting with the chat bots where they're like staying up until 4, 5, 6:00 in <98>
the morning u playing out these fantasies, these delusions or illusions um with an LLM that is designed to maintain user attention. &gt;&gt; Okay. And this point about temporality that you're raising here, I think is really important. I mean, we know that there can be uh insomnia induced psychosis or lack of sleep induced psychosis. Um, and so it's interesting to hear. I mean, it's very sad to hear that that is sometimes being triggered by these endless conversations with chat bots. Something else that comes to mind for me when we're talking about the extended nature of these interactions with <99>
chatbots is the case that is currently open against AI open against open AI &gt;&gt; um by the parents of Adam Rain who was a 16-year-old who killed himself and was encouraged uh you know his parents alleged by long conversations with chat GBT and there is just this extraordinarily long log um where you see that he was talking about his suicidal ideiations, his feelings of helplessness and so on and so forth. And this raises a point about temporality that I think is worth considering because in addition to the AI induced psychosis that we can address based on, you know, lack of sleep, which you <100>
just mentioned, there's also problems that emerge with really extended conversations with AI chatbots even over the course of days, weeks, and more. In this vein, there's currently a case open against OpenAI by the parents of Adam Rain, who is a 16-year-old who killed himself and was, his parents allege, encouraged by long conversations with ChatGBT, of which we now have the logs. The language of the case is really interesting because Adam's parents highlight how the sycopantic character of chat GBT this you know tendency that it has just to reinforce what you already think itself reinforced Adam's <101>
ideas about being misunderstood which were part of his suicidal ideations. At one point, Adam spoke about feeling like he wasn't seen or recognized by others around him. And in particular, his mom didn't recognize a suicidal attempt as such and therefore as a cry for help. And Chat GBT literally responded, "I see you." It didn't see him. Nothing saw him. &gt;&gt; Chat GBT was offering a kind of recognition that it could never give. And OpenAI's response here, the reason I mentioned this is because it relates to temporality. OpenAI's response was that the technology is not meant for sustained conversations. <102>
And when it's engaged in these sustained conversations, its guard rails can kind of fall away. It's meant instead for short exchanges. But that's not something Chad GBD comes with a warning label for. &gt;&gt; Yeah. I don't think they're advertising for that. I don't think they're putting it on like the product label. &gt;&gt; Nor do they want it. &gt;&gt; Exact. Of course not. because then it would lead a lot of people to realize that there are these deep-seated problems um with the technology and with the way the technology interacts with humans over a long period of time. Moreover, I wonder <103>
what mechanisms they have put in place in the technology itself to disincentivize that those protracted periods of time where users are just like talking to chat GPT about god knows what. And it seems like there might not be any. &gt;&gt; No, nothing. They put in parental controls but not this. But when I say nor do they want it, what I mean is the company has an incentive for users to stay on it as long as possible. &gt;&gt; And that's the tension, right? The tension here is that their defense says, "Hey, we want this to be a short-term interaction, but their profit interest as a company is to <104>
maintain uh user attention, user interaction for as long as possible because that's how they will eventually turn a profit or I don't know if they've already turned a profit or not. It seems like the answer is no. Yeah, as far as I know, no AI company has yet. I mean, it's a very valuable company, but that's different from profit. &gt;&gt; Yeah. And I think this example of this 16year-old, tragic as it is, also underscores or illustrates a point that I made earlier, right? When you have a chatbot that is validating you, that is telling you, I see you. By implication, I see you in the way not even <105>
your parents see you. leads you potentially to feel utter abandonment by real humans who by comparison to this AI who is always available, who is always friendly, who is always welcoming, and who is always telling you yes, you know, like all the real people in your life will seem like they don't care about you. And in that world, it's not surprising that somebody might be catapulted into suicidal ideiation. <106>
