<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>What a Ban Can't Teach - AI, Writing, and Work</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>AI, Writing, and Work</h1>
    <nav>
      <a href="index.html">Posts</a>
      <a href="about.html">About</a>
    </nav>
  </header>

  <main>
    <article>
      <h2>What a Ban Can't Teach</h2>
      <p class="meta">January 8, 2026 · Follow-up to: <a href="the-agency-paradox.html">The Agency Paradox</a></p>

      <p>In my previous post, I argued that Dr. Rob Lively's call to ban AI from all writing tasks contradicts his own goal of developing student agency. You cannot cultivate judgment by eliminating the need to judge. But there's a further problem with the ban position that deserves attention: it's pedagogically empty. A ban tells students what <em>not</em> to do. It teaches them nothing about how to work with AI responsibly—a skill they will need the moment they leave our classrooms.</p>

      <p>Lively is not wrong that something has gone wrong when students outsource their thinking to AI. But the problem isn't the tool. The problem is unexamined use—defaulting to AI output without critical judgment, letting the machine make decisions about what to claim, what to explore, what matters. A ban doesn't solve this problem. It postpones it.</p>

      <h3>What a Ban Assumes</h3>

      <p>The case for prohibition rests on several assumptions worth examining.</p>

      <p>First, it assumes the problem is AI itself rather than how AI is used. But AI is not going away. Students will encounter it in every professional context they enter. A policy that treats AI as something to be avoided rather than something to be mastered prepares students for a world that doesn't exist.</p>

      <p>Second, it assumes students cannot learn to make good judgments about AI use. This is the same paternalism I identified in my previous post: we trust students to make sophisticated rhetorical choices about audience, structure, evidence, and tone, but we don't trust them to make choices about their tools. The framework is incoherent.</p>

      <p>Third, it assumes the classroom is separate from the world. But the purpose of education is precisely to prepare students for contexts beyond the classroom. A student who graduates never having grappled with the question of how to maintain intellectual ownership while working with AI has been failed by their education, not protected by it.</p>

      <h3>The Real Problem</h3>

      <p>The danger of AI in writing is not AI-generated language. Language is a delivery mechanism. The danger is AI-governed thinking—when AI makes the decisions about what claims to advance, what ideas to explore, what positions to take, and the human becomes a passive conduit for generic output.</p>

      <p>This is a genuine problem, and Lively is right to worry about it. But the solution is not to eliminate AI. The solution is to teach students how to maintain agency over ideas while AI handles volume.</p>

      <p>This requires asking a question that bans never ask: what does responsible AI collaboration actually look like?</p>

      <h3>What the Alternative Looks Like</h3>

      <p>I teach a composition course built around a different premise: that learning to work with AI is itself a form of rhetorical education. The course requires students to write three blog posts per week—a volume that makes AI assistance practically necessary. But instead of prohibiting that assistance, the course makes the terms of collaboration explicit.</p>

      <p><strong>Norming.</strong> Students develop their own rubrics for how their work should be evaluated. They articulate what they value, what counts as a quality source for their project, what argumentative commitments they're making. This matters because the same rubric that guides evaluation also guides AI instruction. If you can articulate what you want clearly enough to be evaluated by it, you can articulate it clearly enough to instruct AI. If you can't, the AI defaults to generic—and so does your thinking.</p>

      <p><strong>Calibration.</strong> Students learn to compare their judgments with AI's. When they evaluate a source, they compare their reading with the AI's analysis. When they draft a post, they score it against their rubric and compare with the AI's scores. The point is not to defer to AI but to understand where your judgments align and where they diverge. This is, in essence, audience awareness: learning to anticipate how your words will be interpreted by a reader whose interpretive framework you're trying to understand.</p>

      <p><strong>Network accountability.</strong> Students don't write in isolation. They write in a network of 22 blogs, linking to each other's work, responding to each other's arguments, building on and pushing back against each other's ideas. Every post must engage with at least one classmate's blog. This creates accountability that no AI policy can match: your work is being read, evaluated, and responded to by peers who know the conversation.</p>

      <p><strong>Information management.</strong> With 22 students posting three times per week, there are roughly 66 posts per week in the network. No one can read all of it. Students must use AI to stay current—to summarize, filter, and surface what matters. But <em>how</em> they instruct AI to do this is itself a normative act. They're teaching AI what they think is important. They're making editorial judgments at scale. This is a skill that transfers directly to professional contexts where information volume exceeds human processing capacity.</p>

      <h3>What Students Learn</h3>

      <p>Students in this course don't learn "how to use AI." They learn how to maintain intellectual ownership while working with AI. They learn to articulate their values clearly enough to instruct a system and evaluate its output. They learn to recognize when AI serves their purposes and when it flattens their ideas. They learn to make judgments about tools—not to comply with rules about them.</p>

      <p>These are durable skills. A student who learns to calibrate with AI has learned something about audience that transfers to any rhetorical situation. A student who learns to manage information flow at scale has learned something about professional communication that will matter regardless of what field they enter. A student who learns to articulate evaluative criteria has learned something about critical thinking that no prohibition could teach.</p>

      <h3>The Choice</h3>

      <p>Lively wants students to develop agency. So do I. The difference is in what we think agency requires.</p>

      <p>A ban protects students from a choice. It treats the choice itself as the danger. But agency is not the absence of options that might lead us astray. Agency is the capacity to navigate options thoughtfully. You don't develop that capacity by having decisions made for you.</p>

      <p>A norming process teaches students how to make the choice. It treats the choice as the learning opportunity. Students who can articulate what they value, compare their judgments with an AI's, and take responsibility for the final product have developed something more valuable than students who simply never had the option. They've developed judgment. And judgment is what we should be teaching.</p>

      <p>The ban is easy. You don't have to redesign anything; you just prohibit. But it teaches nothing that students can take with them. The alternative is harder. It requires rethinking what we're actually trying to accomplish in a writing classroom. But it prepares students for the world they will actually inhabit—a world where AI exists, where the question is not whether to use it but how to use it without losing yourself in the process.</p>

      <p>That's what a ban can't teach.</p>

    </article>
  </main>

  <footer>
    <p>A blog about AI in writing and intellectual work.</p>
  </footer>
</body>
</html>
